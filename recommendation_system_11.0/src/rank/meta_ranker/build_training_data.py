# src/rank/meta_ranker/build_training_data.py

import os
import pandas as pd
import json
import logging

# -------- Logger --------
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

# -------- Config --------
OUTPUT_DIR = "output/meta_ranker"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ä¸‰ä¸ªå¬å›ç»“æœ
ITEMCF_PATH = "output/item_cf/itemcf_recall.csv"
KEYWORD_PATH = "output/keyword/keyword_hybrid_recall.csv"
DNN_PATH = "output/youtube_dnn/youtube_dnn_faiss_recall.csv"

# ç”¨æˆ·ç”»åƒ
USER_PROFILE_PATH = "data/processed/user_profile_feature.csv"

# è¡Œä¸ºæ—¥å¿—ï¼ˆåš ground truthï¼‰
BEHAVIOR_CSV = "data/raw/user_behavior_log_info.csv"
TOP_K = 50
CUTOFF_DAYS = 7   # Ground truth çª—å£
TRAIN_DAYS = 14   # è®­ç»ƒå¬å›ç»“æœçª—å£ï¼ˆåœ¨labelçª—å£ä¹‹å‰ï¼‰
NEG_POS_RATIO = 5  # âš ï¸ æ¯ä¸ªæ­£æ ·æœ¬é…å¤šå°‘è´Ÿæ ·æœ¬


def load_ground_truth(behavior_csv, cutoff_days=CUTOFF_DAYS):
    """æ„é€ æœ€å7å¤©çš„ç‚¹å‡»ä½œä¸ºæ­£æ ·æœ¬"""
    from src.data.load_behavior_log import build_datetime

    df = pd.read_csv(behavior_csv)
    df["dt"] = build_datetime(df["year"], df["time_stamp"], df["timestamp"])
    max_dt = df["dt"].max()
    min_dt = max_dt - pd.Timedelta(days=cutoff_days)

    df = df[(df["dt"] >= min_dt) & (df["dt"] <= max_dt)]
    df = df[df["action_type"].str.lower() == "click"]

    df["user_id"] = df["user_id"].astype(str)
    df["item_id"] = df["item_id"].astype(str)

    gt = set(zip(df["user_id"], df["item_id"]))
    logger.info(f"âœ… Ground truth æ ·æœ¬æ•°: {len(gt)}")
    return gt


def load_recall_results_with_time_split():
    """åŠ è½½å¬å›ç»“æœï¼Œç¡®ä¿æ—¶é—´çª—å£ä¸é‡å """
    from src.data.load_behavior_log import build_datetime
    
    # è¯»å–è¡Œä¸ºæ—¥å¿—è·å–æ—¶é—´ä¿¡æ¯
    behavior_df = pd.read_csv(BEHAVIOR_CSV)
    behavior_df["dt"] = build_datetime(behavior_df["year"], behavior_df["time_stamp"], behavior_df["timestamp"])
    max_dt = behavior_df["dt"].max()
    
    # è®¡ç®—å¬å›ç»“æœçš„æ—¶é—´çª—å£ï¼ˆåœ¨labelçª—å£ä¹‹å‰ï¼‰
    recall_end_dt = max_dt - pd.Timedelta(days=CUTOFF_DAYS)
    recall_start_dt = recall_end_dt - pd.Timedelta(days=TRAIN_DAYS)
    
    logger.info(f"ğŸ“… Recall window: {recall_start_dt} to {recall_end_dt}")
    logger.info(f"ğŸ“… Label window: {max_dt - pd.Timedelta(days=CUTOFF_DAYS)} to {max_dt}")
    
    dfs = []
    for path, source in [(ITEMCF_PATH, "itemcf"),
                         (KEYWORD_PATH, "keyword"),
                         (DNN_PATH, "youtube_dnn")]:
        if not os.path.exists(path):
            logger.warning(f"âš ï¸ Recall file missing: {path}")
            continue
        df = pd.read_csv(path)
        df["user_id"] = df["user_id"].astype(str)
        df["item_id"] = df["item_id"].astype(str)
        df = df[["user_id", "item_id", "score"]].rename(columns={"score": f"score_{source}"})
        dfs.append(df)

    # å¤–è¿æ¥ï¼Œä¿è¯æ²¡å‘½ä¸­çš„åœ°æ–¹æ˜¯ NaN â†’ 0
    from functools import reduce
    merged = reduce(lambda left, right: pd.merge(left, right, on=["user_id", "item_id"], how="outer"), dfs)
    merged = merged.fillna(0.0)
    return merged


def normalize_scores(df):
    """å¯¹å¬å›åˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–å¤„ç†"""
    score_cols = [col for col in df.columns if col.startswith('score_')]
    
    for col in score_cols:
        if df[col].max() > df[col].min():  # é¿å…é™¤é›¶
            df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())
        else:
            df[col] = 0.0  # å¦‚æœæ‰€æœ‰å€¼ç›¸åŒï¼Œè®¾ä¸º0
    
    logger.info(f"âœ… Normalized score columns: {score_cols}")
    return df


def handle_missing_features(df):
    """å¤„ç†ç‰¹å¾ç¼ºå¤±å€¼ - å‚è€ƒ load_user_info.py çš„å¤„ç†æ–¹å¼"""
    # æ•°å€¼ç‰¹å¾ï¼šæ ¹æ®ä¸šåŠ¡é€»è¾‘é€‰æ‹©åˆé€‚çš„å¡«å……ç­–ç•¥
    numerical_cols = ['age_range', 'recency', 'frequency', 'actions_per_active_day_30d', 
                     'monetary', 'rfm_score']
    
    for col in numerical_cols:
        if col in df.columns:
            if df[col].isna().any():
                if col == 'age_range':
                    # age_range: ç¼ºå¤±ç”¨0å¡«å……ï¼Œè¡¨ç¤ºæœªçŸ¥å¹´é¾„ï¼ˆå‚è€ƒ load_user_info.pyï¼‰
                    df[col] = df[col].fillna(0)
                    logger.info(f"âœ… Filled missing age_range with 0 (unknown age)")
                elif col == 'recency':
                    # recency: ç¼ºå¤±è¡¨ç¤ºä»æœªæ´»è·ƒï¼Œç”¨æ¯”ç°æœ‰æœ€å¤§å€¼æ›´å¤§çš„å€¼å¡«å……
                    # è¿™æ ·åœ¨RFMåˆ†æä¸­ä¼šè¢«å½’ç±»ä¸ºæœ€ä¸æ´»è·ƒçš„ç”¨æˆ·
                    max_recency = df[col].max()
                    if pd.isna(max_recency):
                        fill_value = 31.0  # é»˜è®¤å€¼
                    else:
                        fill_value = max_recency + 1.0  # æ¯”æœ€å¤§å€¼å¤§1
                    df[col] = df[col].fillna(fill_value)
                    logger.info(f"âœ… Filled missing recency with {fill_value} (never active, worse than max={max_recency})")
                elif col in ['frequency', 'monetary', 'rfm_score']:
                    # è¿™äº›ç‰¹å¾ç¼ºå¤±è¡¨ç¤ºæ²¡æœ‰ç›¸å…³è¡Œä¸ºï¼Œç”¨0å¡«å……
                    df[col] = df[col].fillna(0.0)
                    logger.info(f"âœ… Filled missing {col} with 0.0 (no activity)")
                else:
                    # å…¶ä»–æ•°å€¼ç‰¹å¾ç”¨å‡å€¼å¡«å……
                    df[col] = df[col].fillna(df[col].mean())
                    logger.info(f"âœ… Filled missing {col} with mean value")
    
    # ç±»åˆ«ç‰¹å¾ï¼šå‚è€ƒ load_user_info.py çš„å¤„ç†æ–¹å¼
    categorical_cols = ['gender', 'city', 'cluster_id']
    
    for col in categorical_cols:
        if col in df.columns:
            if df[col].isna().any():
                if col == 'gender':
                    # gender: ç¼ºå¤±ç”¨2å¡«å……ï¼Œè¡¨ç¤ºæœªçŸ¥æ€§åˆ«ï¼ˆå‚è€ƒ load_user_info.pyï¼‰
                    df[col] = df[col].fillna(2)
                    logger.info(f"âœ… Filled missing gender with 2 (unknown gender)")
                elif col == 'cluster_id':
                    # cluster_id: ç¼ºå¤±ç”¨-1å¡«å……ï¼Œè¡¨ç¤ºæœªçŸ¥èšç±»
                    df[col] = df[col].fillna(-1)
                    logger.info(f"âœ… Filled missing cluster_id with -1 (unknown cluster)")
                else:
                    # city: ç¼ºå¤±ç”¨'unknown'å¡«å……
                    df[col] = df[col].fillna('unknown')
                    logger.info(f"âœ… Filled missing {col} with 'unknown'")
    
    return df


def build_training_samples():
    """æ„é€ è®­ç»ƒæ ·æœ¬"""
    gt = load_ground_truth(BEHAVIOR_CSV)
    recall_df = load_recall_results_with_time_split()  # ä½¿ç”¨æ—¶é—´åˆ†å‰²ç‰ˆæœ¬

    # å¯¹å¬å›åˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–
    recall_df = normalize_scores(recall_df)

    # åŠ å…¥ label
    recall_df["label"] = recall_df.apply(
        lambda row: 1 if (row["user_id"], row["item_id"]) in gt else 0,
        axis=1
    )

    # âš ï¸ åšè´Ÿé‡‡æ ·
    positives = recall_df[recall_df["label"] == 1]
    negatives = recall_df[recall_df["label"] == 0]

    neg_sample_size = min(len(negatives), len(positives) * NEG_POS_RATIO)
    negatives_sampled = negatives.sample(n=neg_sample_size, random_state=42)

    balanced = pd.concat([positives, negatives_sampled], ignore_index=True)
    logger.info(f"âœ… Balanced dataset: {len(positives)} positives, {len(negatives_sampled)} negatives")

    # åŠ å…¥ç”¨æˆ·ç”»åƒ
    profile = pd.read_csv(USER_PROFILE_PATH)
    profile["user_id"] = profile["user_id"].astype(str)

    # âš ï¸ æ–°å¢ï¼šæŠŠ city è½¬æ¢æˆç±»åˆ« IDï¼ˆæ•´æ•°ï¼‰
    if "city" in profile.columns:
        profile["city"] = profile["city"].astype("category").cat.codes

    keep_cols = ["user_id", "age_range", "gender", "city", "cluster_id",
                 "recency", "frequency", "actions_per_active_day_30d",
                 "monetary", "rfm_score"]

    profile = profile[keep_cols]
    merged = balanced.merge(profile, on="user_id", how="left")
    
    # å¤„ç†ç‰¹å¾ç¼ºå¤±å€¼
    merged = handle_missing_features(merged)

    # ğŸ” ç»Ÿè®¡æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
    pos_count = (merged["label"] == 1).sum()
    neg_count = (merged["label"] == 0).sum()
    logger.info(f"ğŸ“Š Label distribution after sampling: "
                f"positives={pos_count}, negatives={neg_count}, "
                f"ratio={pos_count/(neg_count+1e-8):.4f}")
    logger.info(f"âœ… Final training samples: {merged.shape}")

    # è½¬æ¢ä¸º JSON list æ ¼å¼
    out_path = os.path.join(OUTPUT_DIR, "training_data.json")
    with open(out_path, "w") as f:
        for row in merged.itertuples(index=False):
            record = {
                "user_id": row.user_id,
                "item_id": row.item_id,
                "label": row.label,
                "scores": {
                    "itemcf": row.score_itemcf,
                    "keyword": row.score_keyword,
                    "youtube_dnn": row.score_youtube_dnn
                },
                "features": {
                    "age_range": row.age_range,
                    "gender": row.gender,
                    "city": row.city,
                    "cluster_id": row.cluster_id,
                    "recency": row.recency,
                    "frequency": row.frequency,
                    "actions_per_active_day_30d": row.actions_per_active_day_30d,
                    "monetary": row.monetary,
                    "rfm_score": row.rfm_score
                }
            }
            f.write(json.dumps(record) + "\n")

    logger.info(f"ğŸ“¦ Training data saved to {out_path}")


if __name__ == "__main__":
    build_training_samples()
