# src/rank/din/train_din_corrected.py - ä½¿ç”¨ä¿®æ­£æ•°æ®é›†çš„è®­ç»ƒè„šæœ¬

import os
import json
import logging
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import numpy as np
import pandas as pd
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc
from collections import defaultdict

from src.rank.din.din_model import DINModel
from src.rank.din.dataset_din import DINDataset

# ----------- Logging -----------
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

# ----------- Config -----------
OUTPUT_DIR = "output/din"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# æ•°æ®è·¯å¾„
RECALL_DATA_PATH = "output/fusion/fusion_recall.csv"
USER_PROFILE_PATH = "data/processed/user_profile_feature.csv"
ITEM_METADATA_PATH = "data/raw/item_metadata.csv"
BEHAVIOR_LOG_PATH = "data/raw/user_behavior_log_info.csv"

# ä¿®æ­£é…ç½® - ä¸å¬å›æ¨¡å‹ä¿æŒä¸€è‡´
CONFIG = {
    "embed_dim": 16,           # æ›´å°çš„åµŒå…¥ç»´åº¦
    "max_seq_len": 10,         # æ›´çŸ­çš„åºåˆ—é•¿åº¦
    "hidden_dims": [32, 16],   # æ›´å°çš„éšè—å±‚
    "learning_rate": 0.001,
    "batch_size": 64,          # æ›´å°çš„æ‰¹æ¬¡
    "num_epochs": 3,           # æ›´å°‘çš„è®­ç»ƒè½®æ•°
    "cutoff_days": 7,          # ä¸å¬å›æ¨¡å‹ä¸€è‡´
    "train_days": 30,          # ä¿®æ­£ï¼šä¸å¬å›æ¨¡å‹ä¸€è‡´
    "neg_pos_ratio": 2,
    "max_samples": 10000,      # å¢åŠ æ ·æœ¬æ•°
    "max_users": 1000          # å¢åŠ ç”¨æˆ·æ•°
}

MODEL_PATH = os.path.join(OUTPUT_DIR, "din_model.pt")
CONFIG_PATH = os.path.join(OUTPUT_DIR, "din_config.json")
LOSS_LOG_PATH = os.path.join(OUTPUT_DIR, "din_loss_log.csv")


def collate_fn(batch):
    """è‡ªå®šä¹‰collateå‡½æ•°å¤„ç†å˜é•¿åºåˆ—"""
    user_cate_feats = defaultdict(list)
    user_numeric_feats = []
    item_ids = []
    seq_cate_feats = defaultdict(list)
    recall_scores = []
    seq_masks = []
    labels = []
    
    for sample in batch:
        # ç”¨æˆ·ç±»åˆ«ç‰¹å¾
        for k, v in sample["user_cate_feats"].items():
            user_cate_feats[k].append(v)
        
        # ç”¨æˆ·æ•°å€¼ç‰¹å¾
        user_numeric_feats.append(sample["user_numeric_feats"])
        
        # ç‰©å“ID
        item_ids.append(sample["item_ids"])
        
        # åºåˆ—ç‰¹å¾
        for k, v in sample["seq_cate_feats"].items():
            seq_cate_feats[k].append(v)
        
        # å¬å›åˆ†æ•°
        recall_scores.append(sample["recall_scores"])
        
        # åºåˆ—mask
        seq_masks.append(sample["seq_mask"])
        
        # æ ‡ç­¾
        labels.append(sample["label"])
    
    # è½¬æ¢ä¸ºtensor
    result = {
        "user_cate_feats": {k: torch.tensor(v, dtype=torch.long) for k, v in user_cate_feats.items()},
        "user_numeric_feats": torch.stack(user_numeric_feats),
        "item_ids": torch.stack(item_ids),
        "seq_cate_feats": {k: torch.stack(v) for k, v in seq_cate_feats.items()},
        "recall_scores": torch.stack(recall_scores),
        "seq_mask": torch.stack(seq_masks),
        "labels": torch.stack(labels)
    }
    
    return result


def evaluate_model(model, dataloader, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch in dataloader:
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            for key in batch:
                if isinstance(batch[key], dict):
                    batch[key] = {k: v.to(device) for k, v in batch[key].items()}
                else:
                    batch[key] = batch[key].to(device)
            
            # å‰å‘ä¼ æ’­
            outputs, _ = model(
                user_cate_feats=batch["user_cate_feats"],
                user_numeric_feats=batch["user_numeric_feats"],
                item_ids=batch["item_ids"],
                seq_cate_feats=batch["seq_cate_feats"],
                recall_scores=batch["recall_scores"],
                seq_mask=batch["seq_mask"]
            )
            
            preds = torch.sigmoid(outputs)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(batch["labels"].cpu().numpy())
    
    # è®¡ç®—æŒ‡æ ‡
    if len(set(all_labels)) > 1:  # ç¡®ä¿æœ‰æ­£è´Ÿæ ·æœ¬
        auc_score = roc_auc_score(all_labels, all_preds)
        precision, recall, _ = precision_recall_curve(all_labels, all_preds)
        pr_auc = auc(recall, precision)
    else:
        auc_score = 0.5
        pr_auc = 0.5
    
    return auc_score, pr_auc, all_preds, all_labels


def train_epoch(model, dataloader, criterion, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    num_batches = 0
    
    for batch in dataloader:
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        for key in batch:
            if isinstance(batch[key], dict):
                batch[key] = {k: v.to(device) for k, v in batch[key].items()}
            else:
                batch[key] = batch[key].to(device)
        
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        outputs, attention_weights = model(
            user_cate_feats=batch["user_cate_feats"],
            user_numeric_feats=batch["user_numeric_feats"],
            item_ids=batch["item_ids"],
            seq_cate_feats=batch["seq_cate_feats"],
            recall_scores=batch["recall_scores"],
            seq_mask=batch["seq_mask"]
        )
        
        # è®¡ç®—æŸå¤±
        loss = criterion(outputs, batch["labels"])
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        num_batches += 1
    
    return total_loss / num_batches


def main():
    logger.info("ğŸš€ Training DIN Model (Corrected Version)...")
    logger.info(f"ğŸ“… Time window: train_days={CONFIG['train_days']}, cutoff_days={CONFIG['cutoff_days']}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Using device: {device}")
    
    # åŠ è½½æ•°æ®é›†
    logger.info("Loading dataset...")
    dataset = DINDataset(
        recall_data_path=RECALL_DATA_PATH,
        user_profile_path=USER_PROFILE_PATH,
        item_metadata_path=ITEM_METADATA_PATH,
        behavior_log_path=BEHAVIOR_LOG_PATH,
        max_seq_len=CONFIG["max_seq_len"],
        cutoff_days=CONFIG["cutoff_days"],
        train_days=CONFIG["train_days"],
        neg_pos_ratio=CONFIG["neg_pos_ratio"],
        max_samples=CONFIG["max_samples"],
        max_users=CONFIG["max_users"]
    )
    
    if len(dataset) == 0:
        logger.error("âŒ No samples generated! Please check your data.")
        return
    
    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=CONFIG["batch_size"], 
        shuffle=True, 
        collate_fn=collate_fn,
        num_workers=0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=CONFIG["batch_size"], 
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=0
    )
    
    logger.info(f"âœ… Train samples: {len(train_dataset)}")
    logger.info(f"âœ… Val samples: {len(val_dataset)}")
    
    # åˆ›å»ºæ¨¡å‹
    model_config = {
        "user_cate_dims": dataset.user_cate_dims,
        "user_numeric_dim": len(dataset.user_numeric_cols),
        "embed_dim": CONFIG["embed_dim"],
        "max_seq_len": CONFIG["max_seq_len"],
        "hidden_dims": CONFIG["hidden_dims"]
    }
    
    model = DINModel(**model_config).to(device)
    logger.info(f"âœ… Model created with {sum(p.numel() for p in model.parameters())} parameters")
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=CONFIG["learning_rate"])
    criterion = nn.BCEWithLogitsLoss()
    
    # è®­ç»ƒå¾ªç¯
    best_auc = 0
    train_losses = []
    val_aucs = []
    
    for epoch in range(CONFIG["num_epochs"]):
        logger.info(f"Epoch {epoch+1}/{CONFIG['num_epochs']}")
        
        # è®­ç»ƒ
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
        train_losses.append(train_loss)
        
        # éªŒè¯
        val_auc, val_pr_auc, val_preds, val_labels = evaluate_model(model, val_loader, device)
        val_aucs.append(val_auc)
        
        logger.info(f"Train Loss: {train_loss:.4f}, Val AUC: {val_auc:.4f}, Val PR-AUC: {val_pr_auc:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_auc > best_auc:
            best_auc = val_auc
            torch.save(model.state_dict(), MODEL_PATH)
            logger.info(f"âœ… New best model saved (AUC: {val_auc:.4f})")
    
    # ä¿å­˜é…ç½®
    with open(CONFIG_PATH, "w") as f:
        json.dump(model_config, f, indent=2)
    
    # ä¿å­˜è®­ç»ƒæ—¥å¿—
    log_df = pd.DataFrame({
        "epoch": range(1, len(train_losses) + 1),
        "train_loss": train_losses,
        "val_auc": val_aucs
    })
    log_df.to_csv(LOSS_LOG_PATH, index=False)
    
    logger.info(f"âœ… Training completed! Best AUC: {best_auc:.4f}")
    logger.info(f"ğŸ“¦ Model saved to: {MODEL_PATH}")
    logger.info(f"ğŸ“¦ Config saved to: {CONFIG_PATH}")
    logger.info(f"ğŸ“¦ Loss log saved to: {LOSS_LOG_PATH}")


if __name__ == "__main__":
    main()
