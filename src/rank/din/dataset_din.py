# src/rank/din/dataset_din_corrected.py - ä¿®æ­£ç‰ˆæœ¬çš„æ•°æ®é›†

import os
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset
import logging
from collections import defaultdict
from src.data.load_behavior_log import build_datetime

logger = logging.getLogger(__name__)


class DINDataset(Dataset):
    """DIN æ¨¡å‹æ•°æ®é›† - ä¿®æ­£ç‰ˆæœ¬"""
    
    def __init__(self, 
                 recall_data_path,
                 user_profile_path,
                 item_metadata_path,
                 behavior_log_path,
                 max_seq_len=20,
                 cutoff_days=7,
                 train_days=30,  # ä¿®æ­£ï¼šä¸å¬å›æ¨¡å‹ä¿æŒä¸€è‡´
                 neg_pos_ratio=2,
                 max_samples=10000,
                 max_users=1000):
        """
        Args:
            recall_data_path: å¬å›ç»“æœæ–‡ä»¶è·¯å¾„
            user_profile_path: ç”¨æˆ·ç”»åƒæ–‡ä»¶è·¯å¾„
            item_metadata_path: ç‰©å“å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            behavior_log_path: è¡Œä¸ºæ—¥å¿—æ–‡ä»¶è·¯å¾„
            max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
            cutoff_days: æ ‡ç­¾çª—å£å¤©æ•°
            train_days: è®­ç»ƒæ•°æ®çª—å£å¤©æ•° (ä¿®æ­£ä¸º30å¤©)
            neg_pos_ratio: è´Ÿæ ·æœ¬æ¯”ä¾‹
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            max_users: æœ€å¤§ç”¨æˆ·æ•°
        """
        self.max_seq_len = max_seq_len
        self.cutoff_days = cutoff_days
        self.train_days = train_days
        self.neg_pos_ratio = neg_pos_ratio
        self.max_samples = max_samples
        self.max_users = max_users
        
        # åŠ è½½æ•°æ®
        self._load_data_corrected(recall_data_path, user_profile_path, item_metadata_path, behavior_log_path)
        
        # æ„å»ºç‰¹å¾æ˜ å°„
        self._build_feature_mappings()
        
        # æ„å»ºè®­ç»ƒæ ·æœ¬
        self._build_samples()
        
    def _load_data_corrected(self, recall_data_path, user_profile_path, item_metadata_path, behavior_log_path):
        """ä¿®æ­£ç‰ˆæœ¬çš„æ•°æ®åŠ è½½ç­–ç•¥"""
        logger.info("Loading data with corrected strategy...")
        
        # 1. åŠ è½½å¬å›æ•°æ®
        logger.info("Step 1: Loading recall data...")
        self.recall_df = pd.read_csv(recall_data_path)
        self.recall_df["user_id"] = self.recall_df["user_id"].astype(str)
        self.recall_df["item_id"] = self.recall_df["item_id"].astype(str)
        
        # é‡‡æ ·ç”¨æˆ·
        unique_users = self.recall_df["user_id"].unique()
        if len(unique_users) > self.max_users:
            sampled_users = np.random.choice(unique_users, size=self.max_users, replace=False)
            self.recall_df = self.recall_df[self.recall_df["user_id"].isin(sampled_users)]
        
        logger.info(f"âœ… Loaded recall data: {self.recall_df.shape}")
        
        # 2. åŠ è½½ç”¨æˆ·ç”»åƒ - å…³é”®ä¿®æ­£ï¼šä½¿ç”¨å¬å›æ•°æ®ä¸­çš„ç”¨æˆ·ID
        logger.info("Step 2: Loading user profile...")
        user_profile_df = pd.read_csv(user_profile_path)
        user_profile_df["user_id"] = user_profile_df["user_id"].astype(str)
        
        # ç”±äºèåˆå¬å›å°†ç”¨æˆ·IDé‡æ–°æ˜ å°„ä¸º1,2,3...ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°åŸå§‹ç”¨æˆ·ID
        # ä»å¬å›æ•°æ®ä¸­è·å–ç”¨æˆ·IDåˆ—è¡¨
        recall_users = set(self.recall_df["user_id"].unique())
        
        # ä»ç”¨æˆ·ç”»åƒä¸­é‡‡æ ·ç›¸åŒæ•°é‡çš„ç”¨æˆ·
        available_users = user_profile_df["user_id"].unique()
        if len(available_users) >= len(recall_users):
            # éšæœºé‡‡æ ·ä¸å¬å›æ•°æ®ç›¸åŒæ•°é‡çš„ç”¨æˆ·
            sampled_profile_users = np.random.choice(
                available_users, 
                size=min(len(recall_users), len(available_users)), 
                replace=False
            )
            self.user_profile_df = user_profile_df[user_profile_df["user_id"].isin(sampled_profile_users)]
        else:
            # å¦‚æœç”¨æˆ·ç”»åƒä¸­çš„ç”¨æˆ·æ•°ä¸å¤Ÿï¼Œä½¿ç”¨æ‰€æœ‰ç”¨æˆ·
            self.user_profile_df = user_profile_df
        
        # åˆ›å»ºç”¨æˆ·IDæ˜ å°„ï¼šå¬å›ç”¨æˆ·ID -> ç”¨æˆ·ç”»åƒç”¨æˆ·ID
        recall_users_list = list(recall_users)
        profile_users_list = list(self.user_profile_df["user_id"].unique())
        
        self.user_id_mapping = {}
        for i, recall_user in enumerate(recall_users_list):
            if i < len(profile_users_list):
                self.user_id_mapping[recall_user] = profile_users_list[i]
        
        logger.info(f"âœ… Created user ID mapping for {len(self.user_id_mapping)} users")
        logger.info(f"âœ… Loaded user profile: {self.user_profile_df.shape}")
        
        # 3. åŠ è½½ç‰©å“å…ƒæ•°æ®
        logger.info("Step 3: Loading item metadata...")
        item_metadata_df = pd.read_csv(item_metadata_path)
        item_metadata_df["item_id"] = item_metadata_df["item_id"].astype(str)
        
        recall_items = set(self.recall_df["item_id"].unique())
        self.item_metadata_df = item_metadata_df[item_metadata_df["item_id"].isin(recall_items)]
        logger.info(f"âœ… Loaded item metadata: {self.item_metadata_df.shape}")
        
        # 4. åŠ è½½è¡Œä¸ºæ—¥å¿— - ä½¿ç”¨æ˜ å°„åçš„ç”¨æˆ·ID
        logger.info("Step 4: Loading behavior log...")
        
        behavior_chunks = []
        chunk_size = 100000
        mapped_behavior_users = set(self.user_id_mapping.values())
        
        for chunk in pd.read_csv(behavior_log_path, chunksize=chunk_size):
            chunk["user_id"] = chunk["user_id"].astype(str)
            chunk = chunk[chunk["user_id"].isin(mapped_behavior_users)]
            if not chunk.empty:
                behavior_chunks.append(chunk)
            if len(behavior_chunks) * chunk_size > self.max_samples * 10:
                break
        
        if behavior_chunks:
            self.behavior_df = pd.concat(behavior_chunks, ignore_index=True)
            self.behavior_df["dt"] = build_datetime(
                self.behavior_df["year"], 
                self.behavior_df["time_stamp"], 
                self.behavior_df["timestamp"]
            )
            self.behavior_df["item_id"] = self.behavior_df["item_id"].astype(str)
            # åªä¿ç•™å¬å›ç»“æœä¸­çš„ç‰©å“
            self.behavior_df = self.behavior_df[self.behavior_df["item_id"].isin(recall_items)]
        else:
            # å¦‚æœæ²¡æœ‰è¡Œä¸ºæ•°æ®ï¼Œåˆ›å»ºç©ºçš„DataFrame
            self.behavior_df = pd.DataFrame(columns=["user_id", "item_id", "action_type", "dt"])
        
        logger.info(f"âœ… Loaded behavior log: {self.behavior_df.shape}")
        
    def _build_feature_mappings(self):
        """æ„å»ºç‰¹å¾æ˜ å°„"""
        logger.info("Building feature mappings...")
        
        # åˆ›å»ºç±»åˆ«ç‰¹å¾ç¼–ç å™¨
        from sklearn.preprocessing import LabelEncoder
        
        # ç”¨æˆ·ç‰¹å¾æ˜ å°„
        self.user_encoders = {}
        self.user_cate_dims = {}
        
        # æ€§åˆ«ç¼–ç 
        self.user_encoders["gender"] = LabelEncoder()
        self.user_profile_df["gender_encoded"] = self.user_encoders["gender"].fit_transform(self.user_profile_df["gender"])
        self.user_cate_dims["gender"] = len(self.user_encoders["gender"].classes_)
        
        # å¹´é¾„èŒƒå›´ç¼–ç 
        self.user_encoders["age_range"] = LabelEncoder()
        self.user_profile_df["age_range_encoded"] = self.user_encoders["age_range"].fit_transform(self.user_profile_df["age_range"])
        self.user_cate_dims["age_range"] = len(self.user_encoders["age_range"].classes_)
        
        # åŸå¸‚ç¼–ç 
        self.user_encoders["city"] = LabelEncoder()
        self.user_profile_df["city_encoded"] = self.user_encoders["city"].fit_transform(self.user_profile_df["city"])
        self.user_cate_dims["city"] = len(self.user_encoders["city"].classes_)
        
        # èšç±»IDç¼–ç 
        self.user_encoders["cluster_id"] = LabelEncoder()
        self.user_profile_df["cluster_id_encoded"] = self.user_encoders["cluster_id"].fit_transform(self.user_profile_df["cluster_id"])
        self.user_cate_dims["cluster_id"] = len(self.user_encoders["cluster_id"].classes_)
        
        # ç‰©å“ç‰¹å¾æ˜ å°„ 
        self.item_encoders = {}
        self.item_cate_dims = {}
        
        # ç”¨æˆ·æ•°å€¼ç‰¹å¾
        self.user_numeric_cols = [
            "recency", "frequency", "monetary", "rfm_score",
            "actions_per_active_day_30d", "morning_ratio_30d",
            "afternoon_ratio_30d", "evening_ratio_30d", "late_night_ratio_30d"
        ]
        
        self.item_numeric_cols = []
        
        # åˆ›å»ºIDæ˜ å°„ - åªä½¿ç”¨å¬å›ç»“æœä¸­çš„ç”¨æˆ·å’Œç‰©å“
        recall_users = set(self.recall_df["user_id"].unique())
        recall_items = set(self.recall_df["item_id"].unique())
        
        # åˆ›å»ºIDæ˜ å°„
        self.user2idx = {uid: idx for idx, uid in enumerate(sorted(recall_users))}
        self.item2idx = {iid: idx for idx, iid in enumerate(sorted(recall_items))}
        
        logger.info(f"âœ… User features: {self.user_cate_dims}")
        logger.info(f"âœ… Item features: {self.item_cate_dims}")
        logger.info(f"âœ… User numeric: {len(self.user_numeric_cols)} features")
        logger.info(f"âœ… Item numeric: {len(self.item_numeric_cols)} features")
        logger.info(f"âœ… Unique users: {len(self.user2idx)}")
        logger.info(f"âœ… Unique items: {len(self.item2idx)}")
        
    def _build_samples(self):
        """æ„å»ºè®­ç»ƒæ ·æœ¬"""
        logger.info("Building training samples...")
        
        # è·å–æ ‡ç­¾çª—å£ - ä½¿ç”¨è¡Œä¸ºæ•°æ®çš„æ—¶é—´èŒƒå›´
        if not self.behavior_df.empty:
            max_dt = self.behavior_df["dt"].max()
            label_start_dt = max_dt - pd.Timedelta(days=self.cutoff_days)
            label_end_dt = max_dt
            
            # è·å–è®­ç»ƒçª—å£ - ä¿®æ­£ä¸º30å¤©
            train_start_dt = label_start_dt - pd.Timedelta(days=self.train_days)
            train_end_dt = label_start_dt
            
            logger.info(f"ğŸ“… Label window: {label_start_dt.date()} to {label_end_dt.date()}")
            logger.info(f"ğŸ“… Train window: {train_start_dt.date()} to {train_end_dt.date()}")
            
            # è·å–æ­£æ ·æœ¬ï¼ˆæ ‡ç­¾çª—å£å†…çš„ç‚¹å‡»ï¼Œä¸”å¿…é¡»åœ¨å¬å›ç»“æœä¸­ï¼‰
            positive_df = self.behavior_df[
                (self.behavior_df["dt"] >= label_start_dt) &
                (self.behavior_df["dt"] <= label_end_dt) &
                (self.behavior_df["action_type"].str.lower() == "click")
            ].copy()
            
            # å°†è¡Œä¸ºæ—¥å¿—ä¸­çš„ç”¨æˆ·IDæ˜ å°„å›å¬å›æ•°æ®ä¸­çš„ç”¨æˆ·ID
            reverse_mapping = {v: k for k, v in self.user_id_mapping.items()}
            positive_df["recall_user_id"] = positive_df["user_id"].map(reverse_mapping)
            positive_df = positive_df.dropna(subset=["recall_user_id"])
            
            # åªä¿ç•™å¬å›ç»“æœä¸­å­˜åœ¨çš„ç”¨æˆ·-ç‰©å“å¯¹
            recall_pairs = set(zip(self.recall_df["user_id"], self.recall_df["item_id"]))
            positive_pairs = set(zip(positive_df["recall_user_id"].astype(str), positive_df["item_id"])) & recall_pairs
            logger.info(f"âœ… Positive samples: {len(positive_pairs)}")
            
            # è·å–ç”¨æˆ·å†å²è¡Œä¸ºåºåˆ—ï¼ˆè®­ç»ƒçª—å£å†…ï¼‰
            self.user_sequences = self._build_user_sequences(train_start_dt, train_end_dt)
        else:
            # å¦‚æœæ²¡æœ‰è¡Œä¸ºæ•°æ®ï¼Œåˆ›å»ºç©ºçš„æ­£æ ·æœ¬
            positive_pairs = set()
            self.user_sequences = {}
            logger.info("âš ï¸  No behavior data available, creating empty positive samples")
        
        # æ„å»ºæ ·æœ¬
        self.samples = []
        
        # æ­£æ ·æœ¬
        for user_id, item_id in positive_pairs:
            if user_id in self.user2idx and item_id in self.item2idx:
                self.samples.append({
                    "user_id": user_id,
                    "item_id": item_id,
                    "label": 1
                })
        
        # è´Ÿæ ·æœ¬ï¼ˆå¬å›ç»“æœä¸­ä½†æœªç‚¹å‡»çš„ç”¨æˆ·-ç‰©å“å¯¹ï¼‰
        recall_pairs = set(zip(self.recall_df["user_id"], self.recall_df["item_id"]))
        negative_candidates = recall_pairs - positive_pairs
        
        # ä¸ºæ¯ä¸ªæ­£æ ·æœ¬é‡‡æ ·è´Ÿæ ·æœ¬
        user_positive_items = defaultdict(set)
        for user_id, item_id in positive_pairs:
            user_positive_items[user_id].add(item_id)
        
        for user_id, positive_items in user_positive_items.items():
            user_negative_candidates = [
                (uid, iid) for uid, iid in negative_candidates 
                if uid == user_id and iid not in positive_items
            ]
            
            # éšæœºé‡‡æ ·è´Ÿæ ·æœ¬
            n_neg = min(len(positive_items) * self.neg_pos_ratio, len(user_negative_candidates))
            if n_neg > 0:
                neg_samples = np.random.choice(
                    len(user_negative_candidates), 
                    size=n_neg, 
                    replace=False
                )
                for idx in neg_samples:
                    user_id, item_id = user_negative_candidates[idx]
                    if user_id in self.user2idx and item_id in self.item2idx:
                        self.samples.append({
                            "user_id": user_id,
                            "item_id": item_id,
                            "label": 0
                        })
        
        # å¦‚æœæ­£æ ·æœ¬å¤ªå°‘ï¼Œä»å¬å›ç»“æœä¸­éšæœºé‡‡æ ·ä¸€äº›ä½œä¸ºè´Ÿæ ·æœ¬
        if len(self.samples) < self.max_samples // 2:
            logger.info("âš ï¸  Too few positive samples, sampling from recall data...")
            recall_pairs_list = list(recall_pairs)
            n_samples = min(self.max_samples - len(self.samples), len(recall_pairs_list))
            sampled_pairs = np.random.choice(len(recall_pairs_list), size=n_samples, replace=False)
            
            for idx in sampled_pairs:
                user_id, item_id = recall_pairs_list[idx]
                if user_id in self.user2idx and item_id in self.item2idx:
                    self.samples.append({
                        "user_id": user_id,
                        "item_id": item_id,
                        "label": 0
                    })
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if len(self.samples) > self.max_samples:
            logger.info(f"âš ï¸  Limiting samples from {len(self.samples)} to {self.max_samples}")
            # ä¿æŒæ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
            pos_samples = [s for s in self.samples if s['label'] == 1]
            neg_samples = [s for s in self.samples if s['label'] == 0]
            
            # æŒ‰æ¯”ä¾‹é‡‡æ ·
            pos_ratio = len(pos_samples) / len(self.samples) if self.samples else 0.5
            n_pos = int(self.max_samples * pos_ratio)
            n_neg = self.max_samples - n_pos
            
            if len(pos_samples) > 0:
                pos_samples = np.random.choice(len(pos_samples), size=min(n_pos, len(pos_samples)), replace=False)
                pos_samples = [self.samples[i] for i in pos_samples]
            else:
                pos_samples = []
                
            if len(neg_samples) > 0:
                neg_samples = np.random.choice(len(neg_samples), size=min(n_neg, len(neg_samples)), replace=False)
                neg_samples = [self.samples[i] for i in neg_samples]
            else:
                neg_samples = []
            
            self.samples = pos_samples + neg_samples
        
        logger.info(f"âœ… Total samples: {len(self.samples)}")
        logger.info(f"âœ… Positive: {sum(1 for s in self.samples if s['label'] == 1)}")
        logger.info(f"âœ… Negative: {sum(1 for s in self.samples if s['label'] == 0)}")
        
    def _build_user_sequences(self, start_dt, end_dt):
        """æ„å»ºç”¨æˆ·è¡Œä¸ºåºåˆ—"""
        logger.info("Building user sequences...")
        
        # è·å–è®­ç»ƒçª—å£å†…çš„è¡Œä¸ºæ•°æ®
        train_behavior = self.behavior_df[
            (self.behavior_df["dt"] >= start_dt) &
            (self.behavior_df["dt"] <= end_dt) &
            (self.behavior_df["action_type"].str.lower() == "click")
        ].copy()
        
        # å°†è¡Œä¸ºæ—¥å¿—ä¸­çš„ç”¨æˆ·IDæ˜ å°„å›å¬å›æ•°æ®ä¸­çš„ç”¨æˆ·ID
        reverse_mapping = {v: k for k, v in self.user_id_mapping.items()}
        train_behavior["recall_user_id"] = train_behavior["user_id"].map(reverse_mapping)
        train_behavior = train_behavior.dropna(subset=["recall_user_id"])
        
        # åªä¿ç•™å¬å›ç»“æœä¸­çš„ç”¨æˆ·å’Œç‰©å“
        recall_users = set(self.recall_df["user_id"].unique())
        recall_items = set(self.recall_df["item_id"].unique())
        train_behavior = train_behavior[
            (train_behavior["recall_user_id"].astype(str).isin(recall_users)) &
            (train_behavior["item_id"].isin(recall_items))
        ]
        
        # æŒ‰ç”¨æˆ·å’Œæ—¶é—´æ’åº
        train_behavior = train_behavior.sort_values(["recall_user_id", "dt"])
        
        user_sequences = {}
        for user_id, group in train_behavior.groupby("recall_user_id"):
            items = group["item_id"].tolist()
            # åªä¿ç•™åœ¨item2idxä¸­çš„ç‰©å“
            valid_items = [item for item in items if item in self.item2idx]
            if valid_items:
                user_sequences[str(user_id)] = valid_items[-self.max_seq_len:]  # ä¿ç•™æœ€è¿‘çš„åºåˆ—
        
        logger.info(f"âœ… Built sequences for {len(user_sequences)} users")
        return user_sequences
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        user_id = sample["user_id"]
        item_id = sample["item_id"]
        label = sample["label"]
        
        # è·å–ç”¨æˆ·ç‰¹å¾ - ä½¿ç”¨æ˜ å°„åçš„ç”¨æˆ·ID
        mapped_user_id = self.user_id_mapping.get(user_id, user_id)
        user_profile = self.user_profile_df[
            self.user_profile_df["user_id"] == mapped_user_id
        ].iloc[0]
        
        user_cate_feats = {
            "gender": int(user_profile["gender_encoded"]),
            "age_range": int(user_profile["age_range_encoded"]),
            "city": int(user_profile["city_encoded"]),
            "cluster_id": int(user_profile["cluster_id_encoded"])
        }
        
        user_numeric_feats = [
            user_profile[col] if pd.notna(user_profile[col]) else 0.0
            for col in self.user_numeric_cols
        ]
        
        # è·å–å¬å›åˆ†æ•°
        recall_scores = self.recall_df[
            (self.recall_df["user_id"] == user_id) &
            (self.recall_df["item_id"] == item_id)
        ]
        
        if not recall_scores.empty:
            cf_score = recall_scores["cf_score"].iloc[0] if "cf_score" in recall_scores.columns else 0.0
            keyword_score = recall_scores["keyword_score"].iloc[0] if "keyword_score" in recall_scores.columns else 0.0
            dnn_score = recall_scores["dnn_score"].iloc[0] if "dnn_score" in recall_scores.columns else 0.0
        else:
            cf_score = keyword_score = dnn_score = 0.0
        
        recall_scores = [cf_score, keyword_score, dnn_score]
        
        # è·å–ç”¨æˆ·å†å²åºåˆ—
        if user_id in self.user_sequences:
            seq_items = self.user_sequences[user_id]
            # å¡«å……åˆ°å›ºå®šé•¿åº¦
            while len(seq_items) < self.max_seq_len:
                seq_items = [0] + seq_items  # ç”¨0å¡«å……
            
            seq_items = seq_items[-self.max_seq_len:]  # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
            
            # ç®€åŒ–çš„åºåˆ—ç‰¹å¾ - åªä½¿ç”¨ç‰©å“IDçš„åµŒå…¥
            seq_cate_feats = {}
            seq_mask = []
            
            for item_id in seq_items:
                if item_id == 0:  # å¡«å……ä½ç½®
                    seq_mask.append(0)
                else:
                    if item_id in self.item2idx:
                        seq_mask.append(1)
                    else:
                        seq_mask.append(0)
        else:
            # æ²¡æœ‰å†å²åºåˆ—
            seq_cate_feats = {}
            seq_mask = [0] * self.max_seq_len
        
        return {
            "user_cate_feats": user_cate_feats,
            "user_numeric_feats": torch.tensor(user_numeric_feats, dtype=torch.float32),
            "item_ids": torch.tensor(self.item2idx[item_id], dtype=torch.long),
            "seq_cate_feats": {k: torch.tensor(v, dtype=torch.long) for k, v in seq_cate_feats.items()},
            "recall_scores": torch.tensor(recall_scores, dtype=torch.float32),
            "seq_mask": torch.tensor(seq_mask, dtype=torch.long),
            "label": torch.tensor(label, dtype=torch.float32)
        }
